# -*- coding: utf-8 -*-
"""Weekend Trip Planner Agent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQI5Olbjk4V-8S3h7tGDDJYcdPaiMrzX
"""

import os
import uuid
import json
import time # For exponential backoff
from typing import Dict, Any, List

# --- IMPORTANT: Configure your API Key ---
# This uses environment variables as required by Kaggle for security.
# Make sure to set GEMINI_API_KEY in your Kaggle Notebook Secrets or environment.
API_KEY = os.environ.get("GEMINI_API_KEY", "")
MODEL_NAME = "gemini-2.5-flash-preview-09-2025"
MAX_RETRIES = 5

# Using a try/except block because we don't know the exact environment.
# If the user is using the ADK, they might use a different way to access the LLM.
try:
    from google import genai
    from google.genai.errors import APIError

    if API_KEY:
        client = genai.Client(api_key=API_KEY)
    else:
        # Fallback to a mock LLM if no API key is set for basic testing
        print("WARNING: GEMINI_API_KEY not found. Using MockLLM for generation.")
        class MockClient:
            def models(self):
                return self
            def generate_content(self, model, contents, config=None, system_instruction=None):
                class MockResponse:
                    @property
                    def text(self):
                        # Simulates a basic JSON action response for the Planner Agent
                        if "determine the next best action" in system_instruction:
                            if "hiking and history" in contents[-1]["parts"][0]["text"]:
                                return '{"action": "search_trip_ideas", "query": "weekend trip ideas with hiking and historical sites in warm climate"}'
                            elif "reject" in contents[-1]["parts"][0]["text"]:
                                return '{"action": "refine_ideas"}'
                            else:
                                return '{"action": "ask_for_preferences"}'

                        # Simulates a formatted response for the Formatter Agent
                        return "Mock LLM Response: Based on the search, I recommend a trip to [City Name]. It offers great historical walking tours and scenic trails. Let me know if you like this suggestion!"
                return MockResponse()
        client = MockClient()

except ImportError:
    print("WARNING: google-genai not installed or accessible. Using MockLLM for all generation.")
    # Implement a mock client if the library cannot be imported at all
    class MockClient:
        # ... (same MockClient implementation as above)
        def models(self): return self
        def generate_content(self, model, contents, config=None, system_instruction=None):
            class MockResponse:
                @property
                def text(self):
                    # Simulates a basic JSON action response for the Planner Agent
                    if "determine the next best action" in system_instruction:
                        if "hiking and history" in contents[-1]["parts"][0]["text"]:
                            return '{"action": "search_trip_ideas", "query": "weekend trip ideas with hiking and historical sites in warm climate"}'
                        elif "reject" in contents[-1]["parts"][0]["text"]:
                            return '{"action": "refine_ideas"}'
                        else:
                            return '{"action": "ask_for_preferences"}'

                    # Simulates a formatted response for the Formatter Agent
                    return "Mock LLM Response: Based on the search, I recommend a trip to [City Name]. It offers great historical walking tours and scenic trails. Let me know if you like this suggestion!"
            return MockResponse()
    client = MockClient()


# --- 1. Tools: Web Search Tool (Mocked for immediate execution) ---

class WebSearchTool:
    """
    Mocks a built-in search tool.
    In a real ADK environment, this would be replaced with a real Google Search tool.
    """
    def search(self, query: str) -> str:
        """Simulates calling a web search API."""
        print(f"\n[TOOL CALLED] Searching: {query}")
        # Placeholder search results
        if "hiking and historical sites" in query.lower():
            return "Raw Search Result: Found some great places: City of Seville, Spain (hiking trails and ancient fortresses), Cusco, Peru (Inca history and mountain hiking), and Charleston, SC (historic ports and coastal hiking). Let's suggest Seville."
        elif "weather" in query.lower():
            return "Raw Search Result: Weather for Seville, Spain: Sunny and 20Â°C. Perfect for a weekend trip."
        else:
            return "Raw Search Result: No specific ideas found for that query."

# --- 2. Sessions & State Management (Concept #3) ---

class TripPlanningSession:
    """Manages the state and history for a single user's trip planning conversation."""
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.conversation_history: List[Dict[str, str]] = []
        self.user_preferences: Dict[str, Any] = {}
        self.current_plan_stage: str = "initial" # Controls agent's high-level behavior

    def add_message(self, role: str, content: str):
        """Adds a message to the conversation history."""
        self.conversation_history.append({"role": role, "content": content})

    def get_history_for_llm(self) -> List[Dict[str, str]]:
        """Formats history for the LLM API call."""
        # The history needs to be in the format: [{"role": "user", "parts": [...]}, ...]
        formatted_history = []
        for msg in self.conversation_history:
            # We treat 'tool' and 'user' inputs as 'user' and 'assistant' inputs as 'model' for the API
            api_role = "user" if msg["role"] in ["user", "tool"] else "model"
            formatted_history.append({"role": api_role, "parts": [{"text": msg["content"]}]})
        return formatted_history

    def update_preferences(self, key: str, value: Any):
        """Updates user preferences based on the conversation."""
        self.user_preferences[key] = value

# --- LLM Helper Functions (With Backoff) ---

def call_gemini_with_backoff(system_instruction: str, history: List[Dict[str, str]], json_mode: bool = False) -> str:
    """Handles the Gemini API call with exponential backoff for reliability."""
    if not API_KEY:
        # If API key is not set, use the MockClient directly (its generate_content is synchronous)
        response = client.generate_content(
            model=MODEL_NAME,
            contents=history,
            system_instruction=system_instruction
        )
        return response.text

    for i in range(MAX_RETRIES):
        try:
            config = {}
            if json_mode:
                 config = {
                    "responseMimeType": "application/json",
                    "responseSchema": {
                        "type": "OBJECT",
                        "properties": {
                            "action": {"type": "STRING", "description": "The determined next step for the agent."},
                            "query": {"type": "STRING", "description": "The search query, if action is 'search_trip_ideas' or 'search_weather'."},
                        }
                    }
                }

            response = client.generate_content(
                model=MODEL_NAME,
                contents=history,
                system_instruction=system_instruction,
                config=config
            )
            return response.text
        except APIError as e:
            print(f"API Error (Attempt {i+1}/{MAX_RETRIES}): {e}")
            if i < MAX_RETRIES - 1:
                sleep_time = 2 ** i
                print(f"Retrying in {sleep_time} seconds...")
                time.sleep(sleep_time)
            else:
                return f"ERROR: Failed to get response after {MAX_RETRIES} attempts due to API issues."
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
            return "ERROR: An unexpected error occurred during LLM call."
    return "ERROR: Agent failed to process request."


# --- 3. Multi-Agent System (Sequential Logic) ---

class WeekendTripPlannerAgent:
    """
    The main orchestrator combining all key concepts.
    It manages the sequential flow between the Planner Agent, the Tool, and the Formatter Agent.
    """
    def __init__(self):
        self.web_search_tool = WebSearchTool()
        self.active_sessions: Dict[str, TripPlanningSession] = {}
        print("Weekend Trip Planner Agent Initialized.")

    def get_or_create_session(self, session_id: str) -> TripPlanningSession:
        """Retrieves or initializes a session for state management."""
        if session_id not in self.active_sessions:
            self.active_sessions[session_id] = TripPlanningSession(session_id)
        return self.active_sessions[session_id]

    def _planner_agent_determine_action(self, session: TripPlanningSession) -> Dict[str, str]:
        """
        [LLM-Powered Agent 1: The Planner]
        Uses the LLM to analyze the state and history to determine the next action.
        """
        system_instruction = f"""
        You are the Planner Agent for a Concierge Trip Planner. Your goal is to determine the next logical step based on the conversation history and the current state.

        Current Plan Stage: {session.current_plan_stage}
        User Preferences: {session.user_preferences}

        Output a clean JSON object with the action. Use 'query' only for search actions.

        Possible actions and required keys:
        1. "ask_for_preferences": To gather more detail (no 'query' needed).
        2. "search_trip_ideas": To find destinations (requires 'query' based on preferences).
        3. "refine_ideas": If the user rejected a previous idea (no 'query' needed).
        4. "finalize_trip": If a destination is agreed upon (no 'query' needed).
        5. "respond_directly": For simple small talk or error handling (no 'query' needed).
        """

        history = session.get_history_for_llm()
        # Add a final prompt to force the LLM to generate the action
        history.append({"role": "user", "parts": [{"text": "Based on the conversation, strictly output the next action as a JSON object, adhering to the provided schema."}]})

        json_output = call_gemini_with_backoff(system_instruction, history, json_mode=True)

        try:
            return json.loads(json_output)
        except json.JSONDecodeError:
            print(f"Planner Agent failed to return valid JSON: {json_output}")
            return {"action": "respond_directly", "response": "I encountered an internal planning error. Can you rephrase your last request?"}

    def _formatter_agent_generate_response(self, session: TripPlanningSession, raw_data: str) -> str:
        """
        [LLM-Powered Agent 2: The Formatter]
        Uses the LLM to transform raw tool output into a polished, user-friendly response.
        """
        system_instruction = f"""
        You are the Formatter Agent. Your task is to take the following raw search data and the user's preferences, and craft a polite, encouraging, and clear trip suggestion for the user.
        Do not mention 'raw data'. Focus on the suggestion and ask for confirmation/feedback.

        User Preferences: {session.user_preferences}
        Raw Search Information to format: {raw_data}
        """

        history = session.get_history_for_llm()
        # Add the raw data as a final user message for context
        history.append({"role": "user", "parts": [{"text": f"Format this raw data into a trip suggestion: {raw_data}"}]})

        return call_gemini_with_backoff(system_instruction, history, json_mode=False)


    def process_user_input(self, user_input: str, session_id: str = "default_user") -> str:
        """
        The core loop that manages the sequential flow.
        """
        session = self.get_or_create_session(session_id)
        session.add_message("user", user_input)

        # Simple preference extraction based on initial input for the first turn
        if session.current_plan_stage == "initial":
            if "hiking" in user_input.lower() or "hike" in user_input.lower():
                session.update_preferences("activities", "hiking")
            if "history" in user_input.lower() or "historical" in user_input.lower():
                session.update_preferences("focus", "historical sites")
            if "weekend trip" in user_input.lower() or "travel" in user_input.lower():
                session.current_plan_stage = "gathering_preferences"

        # --- Sequential Agent Execution Loop ---
        while True:
            # Planner Agent determines the next action
            action_data = self._planner_agent_determine_action(session)
            action = action_data.get("action", "respond_directly")

            if action == "search_trip_ideas":
                # Execute Tool (WebSearchTool)
                search_query = action_data.get("query", f"weekend trip ideas for {session.user_preferences}")
                raw_results = self.web_search_tool.search(search_query)
                session.add_message("tool", f"Tool Output: {raw_results}")

                # Update state
                session.current_plan_stage = "presenting_ideas"

                # Execute Formatter Agent
                final_response = self._formatter_agent_generate_response(session, raw_results)
                break

            elif action == "ask_for_preferences":
                # The Planner Agent determined more info is needed
                final_response = "Great idea! To get started on your weekend trip, could you tell me your main interests, like activities or the type of location you prefer (e.g., city, beach, mountains)?"
                break

            elif action == "refine_ideas":
                # The Planner Agent determined the user rejected a previous idea
                final_response = "I understand. Let's pivot! What specifically didn't work for you in the last suggestion (e.g., too cold, too expensive, too far)? This will help me narrow down the perfect spot."
                session.current_plan_stage = "gathering_preferences"
                break

            elif action == "finalize_trip":
                final_response = "Fantastic! We're ready to finalize your trip. Do you need help with a basic itinerary or finding accommodation?"
                break

            else: # Default or respond_directly action
                final_response = action_data.get("response", "I'm still processing your request. What's the most important thing you want to achieve with this trip?")
                break

        session.add_message("assistant", final_response)
        return final_response

# --- Example Run ---
if __name__ == "__main__":

    agent = WeekendTripPlannerAgent()
    user_id = "test_user_123" # Use a unique ID for the session

    print("\n--- TEST CONVERSATION START (User: Hiking & History) ---")

    # Turn 1: User initiates and gives some preferences
    u1 = "I want to plan a weekend trip. I love hiking and history."
    r1 = agent.process_user_input(u1, user_id)
    print(f"\nUSER: {u1}")
    print(f"\nAGENT: {r1}")

    # Turn 2: User rejects the suggestion (simulating rejection of the 'mountainous' part)
    u2 = "That's a good start, but I need something a bit closer and less mountainous. Reject those ideas."
    r2 = agent.process_user_input(u2, user_id)
    print(f"\nUSER: {u2}")
    print(f"\nAGENT: {r2}")

    # Turn 3: User confirms. The agent should now be back to searching or presenting new ideas
    u3 = "Okay, let's try searching again, but this time for something coastal."
    r3 = agent.process_user_input(u3, user_id)
    print(f"\nUSER: {u3}")
    print(f"\nAGENT: {r3}")

    print("\n--- TEST CONVERSATION END ---")